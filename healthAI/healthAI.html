<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech to Text</title>
    <link rel="stylesheet" href="../healthAI/healthAIStyle.css">
</head>
<body>
<a href="../checkIn/selectEmotion/selectColour.html" class="back-button">&#x2B05;</a>
<div id="content">
    <h1>How do you feel today?</h1>
    <p>Click the button, then speak into your microphone.</p>
    <div class="container">
        <div id="chat-container"></div>
        <button id="start-btn"></button>
        <input type="text" id="output" placeholder="Transcribed text will appear here...">
        <button id="save-btn">Save</button>
    </div>
</div>
<div class="quit-to-checkin" id="quit">
    <a href="../index.html" class="quit-to-checkin">Quit</a>
</div>
<script>
    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
        const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        let isListening = false;
        let questionIndex = 0;
        const questions = [
            "How's your energy level today?",
            "Any aches or pains bothering you?",
            "Feeling rested or fatigued?",
            "Are you feeling happy, sad, or somewhere in between?",
            "How's your mood overall?",
            "How clear is your mind today?",
            "Any stress or anxiety on your mind?",
            "Feeling focused or scattered?",
            "Been enjoying company or feeling isolated?",
            "Any interesting interactions lately?",
            "Looking forward to any social events?",
            "Making progress on any tasks or projects?",
            "Engaged in any hobbies or activities today?",
            "Feeling accomplished or overwhelmed?",
            "How's the weather influencing your mood?",
            "Any external factors impacting how you feel?",
            "Comfortable in your surroundings?"
        ];

        // Set recognition language
        recognition.lang = 'en-US';

        // Reference to the output text box and chat container
        const outputTextBox = document.getElementById('output');
        const chatContainer = document.getElementById('chat-container');

        // Function to display a message in the chat
        function displayMessage(message, isUser = false) {
            const messageDiv = document.createElement('div');
            messageDiv.className = isUser ? 'user-message' : 'text-bubble';
            messageDiv.textContent = message;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight; // Auto-scroll to the bottom
        }

        // Function to display a question in the chat
        function displayQuestion(question) {
            displayMessage(question);
        }

        // Event handler for when speech recognition starts
        recognition.onstart = function () {
            outputTextBox.value = 'Listening...';
        };

        // Event handler for when speech recognition results are available
        recognition.onresult = function (event) {
            const transcript = event.results[0][0].transcript;
            outputTextBox.value = transcript; // Set the value of the text box

            // Display the user's response
            displayMessage(transcript, true);

            // Schedule the next question to be displayed after a delay
            if (questionIndex < questions.length) {
                setTimeout(function () {
                    const nextQuestion = questions[questionIndex];
                    displayQuestion(nextQuestion);
                    questionIndex++;
                }, 3000); // 3000 milliseconds (3 seconds) delay
            }
        };


        // Event handler for speech recognition errors
        recognition.onerror = function (event) {
            outputTextBox.value = 'Error occurred: ' + event.error;
        };

        // Event handler for when speech recognition ends
        recognition.onend = function () {
            document.getElementById('start-btn').style.backgroundImage = "url('../images/mic.png')"; // Change background to microphone icon
            isListening = false;
        };

        // Function to toggle speech recognition
        function toggleRecognition() {
            if (isListening) {
                recognition.stop();
                isListening = false;
                document.getElementById('start-btn').style.backgroundImage = "url('../images/mic.png')"; // Change background to microphone icon
                outputTextBox.value = 'Recognition stopped.';
            } else {
                recognition.start();
                isListening = true;
                document.getElementById('start-btn').style.backgroundImage = "url('../images/mic.png')"; // Change background to active microphone icon
                outputTextBox.value = 'Listening...';
            }
        }

        function saveText() {
            const textToSave = outputTextBox.value;
            localStorage.setItem('transcribedText', textToSave);
            alert('Text saved to local storage!');
        }

        // Event handler for button click to toggle speech recognition
        document.getElementById('start-btn').addEventListener('click', toggleRecognition);
        document.getElementById('save-btn').addEventListener('click', saveText);
    } else {
        document.getElementById('output').value = 'Speech recognition is not supported in your browser.';
    }
</script>
</body>
</html>
